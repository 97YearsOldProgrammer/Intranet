"""Simple training script for the IntraNet CNN+BLSTM model."""

import argparse
import math
from pathlib import Path

import torch
import torch.nn as nn

import intranet as it

parser = argparse.ArgumentParser(description="Train the IntraNet CNN+BLSTM model on tokenised genomic data.")
parser.add_argument("corpus", type=str,
    help="Path to the tokenised corpus generated by data2token.py.")
parser.add_argument("labels", type=str,
    help="Path to the label sequence file aligned with the corpus.")
parser.add_argument("embeddings", type=str,
    help="Path to the pre-trained embedding matrix (GloVe format).")
parser.add_argument("--epochs", type=int, default=10,
    help="Number of training epochs [%(default)d].")
parser.add_argument("--batch-size", type=int, default=16,
    help="Mini-batch size [%(default)d].")
parser.add_argument("--lr", type=float, default=1e-3,
    help="Learning rate for Adam optimiser [%(default)g].")
parser.add_argument("--test-split", type=float, default=0.2,
    help="Fraction of data reserved for evaluation [%(default).2f].")
parser.add_argument("--num-workers", type=int, default=0,
    help="Number of worker processes for data loading [%(default)d].")
parser.add_argument("--seed", type=int, default=42,
    help="Random seed for dataset splitting [%(default)d].")
parser.add_argument("--save-path", type=str, default=None,
    help="Optional destination to store the trained model weights.")
args = parser.parse_args()



# Detect Trainning Device
if torch.cuda.is_available():
    device = torch.device("cuda")
    print("Using CUDA (NVIDIA GPU)")
elif torch.backends.mps.is_available():
    device = torch.device("mps")
    print("Using MPS (Apple Silicon)")
else:
    device = torch.device("cpu")
    print("Using CPU")

# Loading embedding vector
embeddings = it.parse_glove_matrix(args.embeddings)
if not embeddings:
    raise ValueError(f"No embeddings parsed from {args.embeddings}.")

sample_vector = next(iter(embeddings.values()))
embedding_dim = sample_vector.shape[-1]

# Prepare Dataset
dataset = it.SplicingDataset(args.corpus, embeddings, args.labels)

unique_labels   = sorted(set(dataset.labels))
num_classes     = len(unique_labels)
if num_classes < 2:
    raise ValueError(
        "At least two classes are required for training. "
        "Verify that the label file encodes multiple feature types."
    )

# Prepare Trainning Set
train_loader, test_loader = it.prepare_dataloaders(
    dataset,
    batch_size=args.batch_size,
    test_split=args.test_split,
    num_workers=args.num_workers,
    seed=args.seed,
)

# Run CNN+BLSTM Model
model = it.IntraNet(embedding_dim=embedding_dim, num_classes=num_classes)
model.to(device)

# Model Stats
total_params        = sum(p.numel() for p in model.parameters())
trainable_params    = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"Total parameters: {total_params:,}")
print(f"Trainable parameters: {trainable_params:,}")

# Trainning
loss_fn     = nn.CrossEntropyLoss(ignore_index=-100)    # ignore label padding of -100
optimiser   = torch.optim.Adam(model.parameters(), lr=args.lr)

for epoch in range(1, args.epochs + 1):
    model.train()
    running_loss = 0.0
    running_examples = 0

    for inputs, targets, lengths in train_loader:
        inputs = inputs.to(device)
        targets = targets.to(device)
        lengths = lengths.to(device)

        optimiser.zero_grad()
        logits = model(inputs, lengths)
        loss = loss_fn(logits, targets)
        loss.backward()
        optimiser.step()

        running_loss += loss.item() * targets.size(0)
        running_examples += targets.size(0)

    train_loss = running_loss / running_examples if running_examples else float("nan")
    test_loss, test_accuracy = it.evaluate(model, test_loader, device, loss_fn)
    
    log_message = f"Epoch {epoch:03d}: train_loss={train_loss:.4f}"
    if not math.isnan(test_loss):
        log_message += f", test_loss={test_loss:.4f}, test_acc={test_accuracy:.4f}"
    print(log_message)

# Save trained model
if args.save_path:
    save_path = Path(args.save_path)
    save_path.parent.mkdir(parents=True, exist_ok=True)
    torch.save(model.state_dict(), save_path)
    print(f"Model checkpoint saved to {save_path}")
